pacf(nokeur_data$Log_Returns^2, main = "PACF of Squared Log Returns")
# squared returns show significant autocorrelation -> we confirm that with the following test:
lags <- 1:10
ljung_box_results <- data.frame(
lag = lags,
Q_statistic = sapply(lags, function(lag) Box.test(
nokeur_data$Log_Returns^2,
lag = lag,
type = "Ljung-Box")$statistic),
p_value = sapply(lags, function(lag) Box.test(
nokeur_data$Log_Returns^2,
lag = lag,
type = "Ljung-Box")$p.value)
)
ljung_box_results
# Build ARIMA model based on ACF/PACF
arima_model <- auto.arima(nokeur_data$Log_Returns)
library(quantmod)
library(ggplot2)
library(tseries)
library(dplyr)
library(rugarch)
library(quantmod)
library(ggplot2)
library(tseries)
library(dplyr)
library(rugarch)
# We retrieve NOK/EUR daily exchange rate data from 2015 to 2025 using Yahoo Finance API
getSymbols("NOKEUR=X", src = "yahoo", from = "2015-01-01", to = "2025-01-01")
# We convert the data to dataframe and adjust column names
nokeur_data <- data.frame(Date = index(`NOKEUR=X`), coredata(`NOKEUR=X`))
nokeur_data <- na.omit(nokeur_data)
colnames(nokeur_data) <- c("Date", "Open", "High", "Low", "Close", "Volume", "Adjusted")
ggplot(nokeur_data, aes(x = Date, y = Close)) +
geom_line(color = "darkorchid4") +
labs(title = "NOK/EUR Exchange Rate",
x = "Date", y = "Exchange Rate") +
theme_minimal()
# Most time-series models require stationary data. Log returns help achieve stationarity.
nokeur_data <- nokeur_data %>%
mutate(Log_Returns = log(Close / lag(Close)))
# Remove NA values caused by lag
nokeur_data <- na.omit(nokeur_data)
# We plot the data to visualy inspect stationarity, volatility clustering,...
ggplot(nokeur_data, aes(x = Date, y = Log_Returns)) +
geom_line(color = "cyan4") +
labs(title = "Log Returns of NOK/EUR Exchange Rate",
x = "Date", y = "Log Returns") +
theme_minimal()
# We confirm stationarity with ADF test (H0 = Time series is non-stationary)
adf.test(nokeur_data$Log_Returns, alternative = "stationary")
# ACF and PACF of Log Returns
par(mfrow = c(1, 2))
acf(nokeur_data$Log_Returns, main = "ACF of Log Returns")
pacf(nokeur_data$Log_Returns, main = "PACF of Log Returns")
# ACF and PACF of squared Log Returns to check for volatility clustering and non-linear dependence
par(mfrow = c(1, 2))
acf(nokeur_data$Log_Returns^2, main = "ACF of Squared Log Returns")
pacf(nokeur_data$Log_Returns^2, main = "PACF of Squared Log Returns")
# squared returns show significant autocorrelation -> we confirm that with the following test:
lags <- 1:10
ljung_box_results <- data.frame(
lag = lags,
Q_statistic = sapply(lags, function(lag) Box.test(
nokeur_data$Log_Returns^2,
lag = lag,
type = "Ljung-Box")$statistic),
p_value = sapply(lags, function(lag) Box.test(
nokeur_data$Log_Returns^2,
lag = lag,
type = "Ljung-Box")$p.value)
)
ljung_box_results
# Build ARIMA model based on ACF/PACF
arima_model <- auto.arima(nokeur_data$Log_Returns)
library(quantmod)
library(ggplot2)
library(tseries)
library(dplyr)
library(rugarch)
library(forecast)
# Build ARIMA model based on ACF/PACF
arima_model <- auto.arima(nokeur_data$Log_Returns)
summary(arima_model)
# Forecast ARIMA
forecasted_values <- forecast(arima_model, h = 30)
plot(forecasted_values)
forecasted_values$
# Build ARIMA model based on ACF/PACF
arima_model <- auto.arima(nokeur_data$Log_Returns)
summary(arima_model)
# Forecast ARIMA
forecasted_values <- forecast(arima_model, h = 30)
plot(forecasted_values)
forecasted_values
#| warning: true
# We define the GARCH model
garch_spec <- ugarchspec(
variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
# "sGARCH" is the standard GARCH model, which models volatility directly.
mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),
distribution.model = "norm",
# We assume the residuals follows a normal distribution.
)
# Calculate log returns
log_returns <- nokeur_data$Log_Returns
# Fitting the GARCH model to the data
garch_fit <- ugarchfit(spec = garch_spec, data = log_returns, solver = "hybrid")
garch_fit
# Forecast volatility for the next 30 days
garch_forecast <- ugarchforecast(garch_fit, n.ahead = 31)
plot(garch_forecast, which = 3)
# Which = 3 plots the predicted volatility (standard deviation) over time.
model <- lm(nokeur_data$Log_Returns ~ 1, nokeur_data)
model_resids <- residuals(model)
# Choose 5 lags (this is daily data, 5 is a full trading week)
arch_test <- FinTS::ArchTest(model_resids, lags = 5)
arch_test
garch_spec_fi <- ugarchspec(
variance.model = list(
model = "fiGARCH",
garchOrder = c(1,1)
),
mean.model = list(
armaOrder = c(0,0),
include.mean = TRUE
),
distribution.model = "norm",
# Giving some starting parameters to help convergence
start.pars = list(omega = 0.0001, alpha1 = 0.2, beta1 = 0.5)
)
garch_fit_fi <- ugarchfit(spec = garch_spec_fi, data = nokeur_data$Log_Returns, solver = "hybrid") #crashes if I run "lbfgs"
garch_fit_fi
garch_spec_ta <- ugarchspec(
variance.model = list(
model = "gjrGARCH",
garchOrder = c(1,1)
),
mean.model = list(
armaOrder = c(0,0),
include.mean = TRUE
),
distribution.model = "norm"
)
garch_fit_ta <- ugarchfit(spec = garch_spec_ta, data = nokeur_data$Log_Returns)
garch_fit_ta
# The functions AIC() and BIC() does not divide by sample size T
# Thus finding the number of observations in order to downscale AIC and BIC
n_obs <- length(nokeur_data$Log_Returns)
# Extracting AIC and BIC for ARIMA
arima_aic <- AIC(arima_model) / n_obs
arima_bic <- BIC(arima_model) / n_obs
# Extracting AIC and BIC for GARCH, fiGARCH, and TARCH
garch_aic <- infocriteria(garch_fit)[1]
garch_bic <- infocriteria(garch_fit)[2]
#garch_criteria <- infocriteria(garch_fit)
#garch_aic <- as.numeric(garch_criteria[1])
#garch_bic <- as.numeric(garch_criteria[2])
aic_fi <- infocriteria(garch_fit_fi)[1]
bic_fi <- infocriteria(garch_fit_fi)[2]
aic_ta <- infocriteria(garch_fit_ta)[1]
bic_ta <- infocriteria(garch_fit_ta)[2]
aic_values_all <- c(arima_aic, garch_aic, aic_fi, aic_ta)
bic_values_all <- c(arima_bic, garch_bic, bic_fi, bic_ta)
model_names_all <- c("ARIMA", "GARCH", "fiGARCH", "TARCH")
aic_bic_comparison_all <- data.frame(
Model = model_names_all,
AIC = aic_values_all,
BIC = bic_values_all
)
print(aic_bic_comparison_all)
library(quantmod)
library(ggplot2)
library(tseries)
library(dplyr)
library(rugarch)
library(forecast)
library(Metrics)
# Extracting actual values (to compare to fitted values)
log_returns <- nokeur_data$Log_Returns
actual_values <- log_returns
# Extracting fitted values from ARIMA, GARCH, fiGARCH and TARCH
arima_fitted <- fitted(arima_model)
garch_fitted <- fitted(garch_fit)
figarch_fitted <- fitted(garch_fit_fi)
tarch_fitted <- fitted(garch_fit_ta)
# Creating a list of all fitted values
fitted_values <- list(
"ARIMA" = arima_fitted,
"GARCH" = garch_fitted,
"fiGARCH" = figarch_fitted,
"TARCH" = tarch_fitted
)
# Computing RMSE and MAE
rmse_values_all <- sapply(fitted_values, function(fit) rmse(actual_values, fit))
mae_values_all <- sapply(fitted_values, function(fit) mae(actual_values, fit))
model_names_all <- c("ARIMA", "GARCH", "fiGARCH", "TARCH")
# Create dataframe
rmse_mae_comparison_all <- data.frame(
Model = model_names_all,
RMSE = rmse_values_all,
MAE = mae_values_all
)
print(rmse_mae_comparison_all)
# Reshaping data for plotting
rmse_mae_long_all <- pivot_longer(rmse_mae_comparison_all, cols = c(RMSE, MAE), names_to = "Metric", values_to = "Value")
library(tidyr)
library(gridExtra)
# Reshaping data for plotting
rmse_mae_long_all <- pivot_longer(rmse_mae_comparison_all, cols = c(RMSE, MAE), names_to = "Metric", values_to = "Value")
# Bar plot comparing RMSE and MAE for all models
ggplot(rmse_mae_long_all, aes(x = Model, y = Value, fill = Metric)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "RMSE and MAE Comparison for All Models", x = "Model", y = "Error Value") +
theme_minimal() +
scale_fill_manual(values = c("RMSE" = "steelblue", "MAE" = "darkorange"))
# Calculate log returns
log_returns <- nokeur_data$Log_Returns
log_returns_train <- head(nokeur_data$Log_Returns, -30)  # Remove the last 30 entries
log_returns_test <- tail(nokeur_data$Log_Returns, 30)
# Fitting the GARCH model to the data
garch_fit_1 <- ugarchfit(spec = garch_spec, data = log_returns_train, solver = "hybrid")
garch_fit
realized_volatility <- as.data.frame(sigma(garch_fit_1))
# Forecast volatility for the next 30 days
garch_forecast <- ugarchforecast(garch_fit_1, n.ahead = 30)
plot(garch_forecast, which = 3)
# which = 3 plots the predicted volatility (standard deviation) over time.
# Extract predicted volatility
predicted_volatility <- as.data.frame(sigma(garch_forecast))
predicted_volatility$Date <- tail(nokeur_data$Date, 30)
colnames(predicted_volatility) <- c("Predicted_Volatility", "Date")
# Define rolling window size (e.g., 30 days)
window_size <- 30
# Compute rolling realized volatility
nokeur_data$Realized_Volatility <- rollapply(nokeur_data$Log_Returns,
width = window_size,
FUN = function(x) sqrt(sum(x^2)),
align = "right", fill = NA)
library(quantmod)
library(ggplot2)
library(tseries)
library(dplyr)
library(rugarch)
library(forecast)
library(Metrics)
library(quantmod)
library(ggplot2)
library(tseries)
library(dplyr)
library(rugarch)
library(forecast)
library(Metrics)
library(zoo)
# Calculate log returns
log_returns <- nokeur_data$Log_Returns
log_returns_train <- head(nokeur_data$Log_Returns, -30)  # Remove the last 30 entries
log_returns_test <- tail(nokeur_data$Log_Returns, 30)
# Fitting the GARCH model to the data
garch_fit_1 <- ugarchfit(spec = garch_spec, data = log_returns_train, solver = "hybrid")
garch_fit
realized_volatility <- as.data.frame(sigma(garch_fit_1))
# Forecast volatility for the next 30 days
garch_forecast <- ugarchforecast(garch_fit_1, n.ahead = 30)
plot(garch_forecast, which = 3)
# which = 3 plots the predicted volatility (standard deviation) over time.
# Extract predicted volatility
predicted_volatility <- as.data.frame(sigma(garch_forecast))
predicted_volatility$Date <- tail(nokeur_data$Date, 30)
colnames(predicted_volatility) <- c("Predicted_Volatility", "Date")
# Define rolling window size (e.g., 30 days)
window_size <- 30
# Compute rolling realized volatility
nokeur_data$Realized_Volatility <- rollapply(nokeur_data$Log_Returns,
width = window_size,
FUN = function(x) sqrt(sum(x^2)),
align = "right", fill = NA)
nokeur_data$Realized_Volatility_Daily <- nokeur_data$Realized_Volatility / sqrt(window_size)
# Merge the data
comparison_data <- merge(nokeur_data, predicted_volatility, by = "Date", all.x = TRUE)
# Set the time period for the last year (e.g., 2024)
start_date <- as.Date("2024-01-01")
end_date <- as.Date("2024-12-31")
# Filter data for the period of interest
comparison_data_last_year <- subset(comparison_data, Date >= start_date & Date <= end_date)
# Plot the comparison
ggplot(comparison_data_last_year, aes(x = Date)) +
geom_line(aes(y = comparison_data_last_year$Realized_Volatility_Daily, color = "Realized Volatility"), size = 1) +
geom_line(aes(y = comparison_data_last_year$Predicted_Volatility, color = "Predicted Volatility"), size = 1) +
labs(title = "Predicted vs. Realized Volatility",
y = "Volatility", x = "Date") +
scale_color_manual(values = c("Realized Volatility" = "blue", "Predicted Volatility" = "red")) +
theme_minimal()
library(quantmod)
library(ggplot2)
library(tseries)
library(dplyr)
library(rugarch)
library(forecast)
library(Metrics)
library(zoo)
library(tidyr)
library(gridExtra)
library(quantmod)
library(ggplot2)
library(tseries)
library(dplyr)
library(rugarch)
library(forecast)
library(Metrics)
library(zoo)
library(tidyr)
library(gridExtra)
# We retrieve NOK/EUR daily exchange rate data from 2015 to 2025 using Yahoo Finance API
getSymbols("NOKEUR=X", src = "yahoo", from = "2015-01-01", to = "2025-01-01")
# We convert the data to dataframe and adjust column names
nokeur_data <- data.frame(Date = index(`NOKEUR=X`), coredata(`NOKEUR=X`))
nokeur_data <- na.omit(nokeur_data)
colnames(nokeur_data) <- c("Date", "Open", "High", "Low", "Close", "Volume", "Adjusted")
ggplot(nokeur_data, aes(x = Date, y = Close)) +
geom_line(color = "darkorchid4") +
labs(title = "NOK/EUR Exchange Rate",
x = "Date", y = "Exchange Rate") +
theme_minimal()
# Most time-series models require stationary data. Log returns help achieve stationarity.
nokeur_data <- nokeur_data %>%
mutate(Log_Returns = log(Close / lag(Close)))
# Remove NA values caused by lag
nokeur_data <- na.omit(nokeur_data)
# We plot the data to visualy inspect stationarity, volatility clustering,...
ggplot(nokeur_data, aes(x = Date, y = Log_Returns)) +
geom_line(color = "cyan4") +
labs(title = "Log Returns of NOK/EUR Exchange Rate",
x = "Date", y = "Log Returns") +
theme_minimal()
# We confirm stationarity with ADF test (H0 = Time series is non-stationary)
adf.test(nokeur_data$Log_Returns, alternative = "stationary")
# ACF and PACF of Log Returns
par(mfrow = c(1, 2))
acf(nokeur_data$Log_Returns, main = "ACF of Log Returns")
pacf(nokeur_data$Log_Returns, main = "PACF of Log Returns")
# ACF and PACF of squared Log Returns to check for volatility clustering and non-linear dependence
par(mfrow = c(1, 2))
acf(nokeur_data$Log_Returns^2, main = "ACF of Squared Log Returns")
pacf(nokeur_data$Log_Returns^2, main = "PACF of Squared Log Returns")
# squared returns show significant autocorrelation -> we confirm that with the following test:
lags <- 1:10
ljung_box_results <- data.frame(
lag = lags,
Q_statistic = sapply(lags, function(lag) Box.test(
nokeur_data$Log_Returns^2,
lag = lag,
type = "Ljung-Box")$statistic),
p_value = sapply(lags, function(lag) Box.test(
nokeur_data$Log_Returns^2,
lag = lag,
type = "Ljung-Box")$p.value)
)
ljung_box_results
# Build ARIMA model based on ACF/PACF
arima_model <- auto.arima(nokeur_data$Log_Returns)
summary(arima_model)
# Forecast ARIMA
forecasted_values <- forecast(arima_model, h = 30)
plot(forecasted_values)
forecasted_values
#| warning: true
# We define the GARCH model
garch_spec <- ugarchspec(
variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
# "sGARCH" is the standard GARCH model, which models volatility directly.
mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),
distribution.model = "norm",
# We assume the residuals follows a normal distribution.
)
# Calculate log returns
log_returns <- nokeur_data$Log_Returns
# Fitting the GARCH model to the data
garch_fit <- ugarchfit(spec = garch_spec, data = log_returns, solver = "hybrid")
garch_fit
# Forecast volatility for the next 30 days
garch_forecast <- ugarchforecast(garch_fit, n.ahead = 30)
plot(garch_forecast, which = 3)
# Which = 3 plots the predicted volatility (standard deviation) over time.
# Calculate log returns
log_returns <- nokeur_data$Log_Returns
log_returns_train <- head(nokeur_data$Log_Returns, -30)  # Remove the last 30 entries
log_returns_test <- tail(nokeur_data$Log_Returns, 30)
# Fitting the GARCH model to the data
garch_fit_1 <- ugarchfit(spec = garch_spec, data = log_returns_train, solver = "hybrid")
garch_fit
realized_volatility <- as.data.frame(sigma(garch_fit_1))
# Forecast volatility for the next 30 days
garch_forecast <- ugarchforecast(garch_fit_1, n.ahead = 30)
plot(garch_forecast, which = 3)
# which = 3 plots the predicted volatility (standard deviation) over time.
# Extract predicted volatility
predicted_volatility <- as.data.frame(sigma(garch_forecast))
predicted_volatility$Date <- tail(nokeur_data$Date, 30)
colnames(predicted_volatility) <- c("Predicted_Volatility", "Date")
# Define rolling window size (e.g., 30 days)
window_size <- 30
# Compute rolling realized volatility
nokeur_data$Realized_Volatility <- rollapply(nokeur_data$Log_Returns,
width = window_size,
FUN = function(x) sqrt(sum(x^2)),
align = "right", fill = NA)
nokeur_data$Realized_Volatility_Daily <- nokeur_data$Realized_Volatility / sqrt(window_size)
# Merge the data
comparison_data <- merge(nokeur_data, predicted_volatility, by = "Date", all.x = TRUE)
# Set the time period for the last year (e.g., 2024)
start_date <- as.Date("2024-01-01")
end_date <- as.Date("2024-12-31")
# Filter data for the period of interest
comparison_data_last_year <- subset(comparison_data, Date >= start_date & Date <= end_date)
# Plot the comparison
ggplot(comparison_data_last_year, aes(x = Date)) +
geom_line(aes(y = comparison_data_last_year$Realized_Volatility_Daily, color = "Realized Volatility"), size = 1) +
geom_line(aes(y = comparison_data_last_year$Predicted_Volatility, color = "Predicted Volatility"), size = 1) +
labs(title = "Predicted vs. Realized Volatility",
y = "Volatility", x = "Date") +
scale_color_manual(values = c("Realized Volatility" = "blue", "Predicted Volatility" = "red")) +
theme_minimal()
model <- lm(nokeur_data$Log_Returns ~ 1, nokeur_data)
model_resids <- residuals(model)
# Choose 5 lags (this is daily data, 5 is a full trading week)
arch_test <- FinTS::ArchTest(model_resids, lags = 5)
arch_test
garch_spec_fi <- ugarchspec(
variance.model = list(
model = "fiGARCH",
garchOrder = c(1,1)
),
mean.model = list(
armaOrder = c(0,0),
include.mean = TRUE
),
distribution.model = "norm",
# Giving some starting parameters to help convergence
start.pars = list(omega = 0.0001, alpha1 = 0.2, beta1 = 0.5)
)
garch_fit_fi <- ugarchfit(spec = garch_spec_fi, data = nokeur_data$Log_Returns, solver = "hybrid") #crashes if I run "lbfgs"
garch_fit_fi
garch_spec_ta <- ugarchspec(
variance.model = list(
model = "gjrGARCH",
garchOrder = c(1,1)
),
mean.model = list(
armaOrder = c(0,0),
include.mean = TRUE
),
distribution.model = "norm"
)
garch_fit_ta <- ugarchfit(spec = garch_spec_ta, data = nokeur_data$Log_Returns)
garch_fit_ta
# The functions AIC() and BIC() does not divide by sample size T
# Thus finding the number of observations in order to downscale AIC and BIC
n_obs <- length(nokeur_data$Log_Returns)
# Extracting AIC and BIC for ARIMA
arima_aic <- AIC(arima_model) / n_obs
arima_bic <- BIC(arima_model) / n_obs
# Extracting AIC and BIC for GARCH, fiGARCH, and TARCH
garch_aic <- infocriteria(garch_fit)[1]
garch_bic <- infocriteria(garch_fit)[2]
aic_fi <- infocriteria(garch_fit_fi)[1]
bic_fi <- infocriteria(garch_fit_fi)[2]
aic_ta <- infocriteria(garch_fit_ta)[1]
bic_ta <- infocriteria(garch_fit_ta)[2]
aic_values_all <- c(arima_aic, garch_aic, aic_fi, aic_ta)
bic_values_all <- c(arima_bic, garch_bic, bic_fi, bic_ta)
model_names_all <- c("ARIMA", "GARCH", "fiGARCH", "TARCH")
aic_bic_comparison_all <- data.frame(
Model = model_names_all,
AIC = aic_values_all,
BIC = bic_values_all
)
print(aic_bic_comparison_all)
# Extracting actual values (to compare to fitted values)
log_returns <- nokeur_data$Log_Returns
actual_values <- log_returns
# Extracting fitted values from ARIMA, GARCH, fiGARCH and TARCH
arima_fitted <- fitted(arima_model)
garch_fitted <- fitted(garch_fit)
figarch_fitted <- fitted(garch_fit_fi)
tarch_fitted <- fitted(garch_fit_ta)
# Creating a list of all fitted values
fitted_values <- list(
"ARIMA" = arima_fitted,
"GARCH" = garch_fitted,
"fiGARCH" = figarch_fitted,
"TARCH" = tarch_fitted
)
# Computing RMSE and MAE
rmse_values_all <- sapply(fitted_values, function(fit) rmse(actual_values, fit))
mae_values_all <- sapply(fitted_values, function(fit) mae(actual_values, fit))
model_names_all <- c("ARIMA", "GARCH", "fiGARCH", "TARCH")
# Create dataframe
rmse_mae_comparison_all <- data.frame(
Model = model_names_all,
RMSE = rmse_values_all,
MAE = mae_values_all
)
print(rmse_mae_comparison_all)
# Reshaping data for plotting
rmse_mae_long_all <- pivot_longer(rmse_mae_comparison_all, cols = c(RMSE, MAE), names_to = "Metric", values_to = "Value")
# Bar plot comparing RMSE and MAE for all models
ggplot(rmse_mae_long_all, aes(x = Model, y = Value, fill = Metric)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "RMSE and MAE Comparison for All Models", x = "Model", y = "Error Value") +
theme_minimal() +
scale_fill_manual(values = c("RMSE" = "steelblue", "MAE" = "darkorange"))

---
title: "ARIMA"
format:
  html:
    toc: true
    toc-location: left
editor: visual
---

# Import and prepare data

```{r}
library(readxl)
UKHP = read_excel("UKHP.xls", col_types = c("date", "numeric")) # series in levels
names(UKHP)[2] = "hp"

UKHP$dhp = c(NA, 100*diff(UKHP$hp)/UKHP$hp[1:nrow(UKHP)-1]) # 1st differenced series


```

## Plot data

```{r}

par(cex.axis = 1.5, cex.lab = 1.5, lwd = 2)
plot(UKHP$Month,UKHP$hp,type = 'l',xlab="Date",ylab="House price")
plot(UKHP$Month,UKHP$dhp,type = 'l',xlab="Date",ylab="% chg, House price")

hist(UKHP$dhp)
box()
```

## Testing for Unit Roots

```{r}
library(fUnitRoots)

adfTest(UKHP$hp,lags = 10,type = "c")

adfTest(UKHP$dhp,lags = 10,type = "c")

adfgls = urersTest(UKHP$hp, type = "DF-GLS", model = "trend", lag.max = 10)
adfgls@test$test@teststat
adfgls@test$test@cval
```

The output shows the actual test statistics for the null hypothesis that the series \`hp' has a unit root. Clearly, the null hypothesis of a unit root in the house price series cannot be rejected with a p-value of 0.903.

We find that the null hypothesis of a unit root can be rejected for the returns on the house price series at the 5% level.

## ACF and PACF

```{r}
UKHP = UKHP[-1,]

ac = acf(UKHP$dhp,lag.max = 12)
pac = pacf(UKHP$dhp,lag.max = 12)
```

It is clearly evident that the series is quite persistent given that it is already in percentage change form: the autocorrelation function dies away rather slowly. Only the first two partial autocorrelation coefficients appear strongly significant.

# Choosing ARMA-specification

```{r}
ar11 = arima(UKHP$dhp,order = c(1,0,1)) # ARMA(1,1)

AIC(ar11)
AIC(ar11, k = log(nrow(UKHP)))
```

```{r}
aic_table = array(NA,c(6,6,2)) #ARMA(p,q)
for (ar in 0:5) {
  for (ma in 0:5) {
    arma = arima(UKHP$dhp,order = c(ar,0,ma))
    aic_table[ar+1,ma+1,1] = AIC(arma) #AIC
    aic_table[ar+1,ma+1,2] = AIC(arma, k = log(nrow(UKHP))) #BIC
  }
}

aic_table[,,1] #AIC
aic_table[,,2] #BIC

which.min(aic_table[,,1]) #AIC
which.min(aic_table[,,2]) #BIC
```

# Forecasting Using ARMA Models

Suppose that an AR(2) model selected for the house price percentage changes series was estimated using observations February 1991{December 2015, leaving 27 remaining observations to construct forecasts for and to test forecast accuracy (for the period January 2016{March 2018). Let us first estimate the ARMA(2,0) model for the time period from February 1991{December 2015.

```{r}
arima(UKHP$dhp[UKHP$Month <="2015-12-01"], order = c(2,0,0))
```

Now that we have fitted the model, we can produce the forecasts for the period January 2016 to March 2018. There are two methods for constructing forecasts: dynamic and static. Dynamic forecasts are multi-step forecasts starting from the first period in the forecast sample. Static forecasts imply a sequence of one-step-ahead forecasts, rolling the sample forwards one observation after each forecast.

```{r}
ar2 = arima(UKHP$dhp[UKHP$Month <="2015-12-01"], order = c(2,0,0))
dynamic_fc = predict(ar2,n.ahead = 27)
static_fc = ar2$coef[3]+ar2$coef[1]*UKHP$dhp[299:325]+ar2$coef[2]*UKHP$dhp[298:324]

par(lwd=2,cex.axis = 2)
plot(UKHP$Month[300:326],UKHP$dhp[300:326],type = "l",xlab = "",ylab = "")
lines(UKHP$Month[300:326],dynamic_fc$pred,col="blue")
lines(UKHP$Month[300:326],static_fc,col="red")
legend("topright", legend=c("Actual", "Dynamic", "Static"),col=c("black","blue","red"),lty= 1)
```

For the dynamic forecasts, it is clearly evident that the forecasts quickly converge upon the long-term unconditional mean value as the horizon increases. Of course, this does not occur with the series of 1-step-ahead forecasts which seem to more closely resemble the actual \`dhp' series.

# Evaluate forecasts

```{r}


# Load necessary library
library(forecast)
library(MCS)  # For DM test function

# Fit AR(2) model
#ar2 <- arima(UKHP$dhp[UKHP$Month <= "2015-12-01"], order = c(2,0,0))

# Ensure actual values are numeric
actual <- as.numeric(UKHP$dhp[300:326])

# Generate Dynamic Forecast (Ensure proper extraction from the list)
dynamic_fc_list <- predict(ar2, n.ahead = 27)  # List output
dynamic_fc <- as.numeric(dynamic_fc_list$pred)  # Extract numeric predictions

# Generate Static Forecast (Ensure it is numeric)
static_fc <- as.numeric(ar2$coef[3] + ar2$coef[1] * UKHP$dhp[299:325] + ar2$coef[2] * UKHP$dhp[298:324])

# Compute MSE, MAE, MAPE
mse_dynamic <- mean((actual - dynamic_fc)^2, na.rm = TRUE)
mae_dynamic <- mean(abs(actual - dynamic_fc), na.rm = TRUE)
mape_dynamic <- mean(abs((actual - dynamic_fc) / actual), na.rm = TRUE) * 100

mse_static <- mean((actual - static_fc)^2, na.rm = TRUE)
mae_static <- mean(abs(actual - static_fc), na.rm = TRUE)
mape_static <- mean(abs((actual - static_fc) / actual), na.rm = TRUE) * 100

# Print results
cat("Dynamic Forecast - MSE:", mse_dynamic, "MAE:", mae_dynamic, "MAPE:", mape_dynamic, "%\n")
cat("Static Forecast - MSE:", mse_static, "MAE:", mae_static, "MAPE:", mape_static, "%\n")

# Perform Diebold-Mariano Test


dm_test <- dm.test((actual - dynamic_fc)^2, (actual - static_fc)^2, alternative = "two.sided", h = 1, power = 2)

# Print DM test results
cat("Diebold-Mariano Test Statistic:", dm_test$statistic, "\n")
cat("p-value:", dm_test$p.value, "\n")

# Interpret Results
if (dm_test$p.value < 0.05) {
  cat("Significant difference in forecast accuracy. Lower error method is better.\n")
} else {
  cat("No significant difference between dynamic and static forecasts.\n")
}

```
